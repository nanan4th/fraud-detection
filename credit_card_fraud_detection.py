# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A2ftfa3akauu5JPqiXLGYX03nm4e5yDJ

# Import Library
"""

!pip install kaggle

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score, ConfusionMatrixDisplay

"""# Load Dataset

Sumber dataset: https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud/metadata
"""

# Load API Token
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

# Import data by API (after upload kaggle.json)
! kaggle datasets download "dhanushnarayananr/credit-card-fraud"

import zipfile
with zipfile.ZipFile("/content/credit-card-fraud.zip","r") as zip_ref:
    zip_ref.extractall("/content")

data = pd.read_csv("card_transdata.csv")
data =  pd.DataFrame(data)
print("data terdiri dari {} baris dan {} kolom".format(data.shape[0], data.shape[1]))

data.head()

"""# EDA

## Data Description

Penjelasan Fitur:
- distance_from_home : jarak dari rumah ke tempat dimana transaksi dilakukan
- distance_from_last_transaction : jarak dari tempat terakhir dilakukannya transaksi
- ratio_to_median_purchase_price : rasio transaksi dengan median harga beli
- repeat_retailer : apakah transaksi terjadi dari retail yang sama
- used_chip : apakah transaksi melalui chip (credit card)
- used_pin_number : apakah transaksi tersebut menggunakan nomor pin
- online_order : apakah transaksi tersebut merupakan pesanan online
- fraud : apakah transaksi tersebut termasuk penipuan (fraud)
"""

data.info()

data.describe()

"""## Handle Noise Data

### Missing Values

Mengecek duplikasi data
"""

if data.duplicated().sum() > 0:
  print('Terdapat redundansi')
else:
  print('Tidak terdapat redundansi')

"""Mengecek apakah ada nilai null"""

print('Jumlah missing value adalah :', data.isnull().sum().sum())

"""Mengecek apakah ada nilai yang tidak sesuai pada data boolean"""

boolean_features = ["repeat_retailer", "used_chip", "used_pin_number", "online_order", "fraud"]

for feature in boolean_features:
  print("unique values from: {}".format(feature))
  print(data[feature].unique())

"""## Univariate"""

data.hist(bins=50, figsize=(20,15))
plt.show()

fraud_0 = (data['fraud'] == 0).sum()
fraud_1 = (data['fraud'] == 1).sum()
print("jumlah fitur fraud bernilai 0 adalah {} dengan persentase {}%".format(fraud_0, (fraud_0/data.shape[0])*100))
print("jumlah fitur fraud bernilai 1 adalah {} dengan persentase {}%".format(fraud_1, (fraud_1/data.shape[0])*100))

"""Dari histogram diatas, kita bisa memahami beberapa informasi, antara lain:
- distribusi data numerikal kontinyu (distance_from_home, distance_from_last_transaction dan ratio_to_median_purchase_price) miring ke kanan atau right-skewed. Hal ini akan berimplikasi pada model sehingga lebih baik dilakukan standarisasi nantinya.
- distribusi data fraud tidak seimbang (imbalance data) sehingga ada baiknya dilakukan undersampling.

## Undersampling

Disini saya akan melakukan undersampling. Saya akan melakukan "Random Under Sampling" dengan menghilangkan data sehingga distribusi datanya bisa lebih seimbang.

### Splitting the data for undersampling

Sebelum melakukan undersampling, saya memisahkan dengan dataframe aslinya terlebih dahulu. Hal ini dilakukan karena nantinya pada saat evaluasi model (evaluation), saya akan menguji model dengan dataframe aslinya bukan dataframe yang dibuat dengan teknik undersampling. Tujuannya untuk menyesuaikan model dengan dataframe sebelum dilakukan undersample atau oversample dan dapat mendeteksi pola lebih baik.
"""

X = data.drop('fraud', axis=1)
y = data['fraud']

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

train_unique_label, train_counts_label = np.unique(y_train, return_counts=True)
test_unique_label, test_counts_label = np.unique(y_test, return_counts=True)

"""### Random Under Sampling"""

df = data.sample(frac=1)

fraud_df = df.loc[df['fraud'] == 1]
non_fraud_df = df.loc[df['fraud'] == 0][:fraud_1]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

new_df = normal_distributed_df.sample(frac=1, random_state=42)

new_df.head()

new_df.hist(bins=50, figsize=(20,15))
plt.show()

"""Ketika distribusi data fraud sudah seimbang, barulah saya melakukan pembersihan data lebih lanjut dengan menghilangkan outlier.

## Handle Outlier
"""

data_numeric = ["distance_from_home", "distance_from_last_transaction", "ratio_to_median_purchase_price"]

sns.boxplot(x = new_df["distance_from_last_transaction"])

sns.boxplot(x = new_df["distance_from_home"])

sns.boxplot(x = new_df["ratio_to_median_purchase_price"])

"""Dari boxplot diatas, dapat diperoleh informasi bahwa terdapat beberapa nilai yang terlalu jauh dari distribusi nilai normalnya. Untuk itu, saya hanya menghilangkan nilai-nilai terakhir (dari quantile terakhir) sehingga distribusi nilainya dapat menjadi lebih baik dan tidak menghilangkan terlalu banyak data. Saya akan menggunakan batas bawah mulai quantile=0 dan batas atas pada quantile=0.99."""

Q1 = new_df.quantile(0)
Q3 = new_df.quantile(0.99)
IQR=Q3-Q1
new_df=new_df[~((new_df<(Q1-1.5*IQR))|(new_df>(Q3+1.5*IQR))).any(axis=1)]
 
# Cek ukuran dataset setelah kita drop outliers
new_df.shape

"""## Multivariate"""

plt.figure(figsize=(10, 10))

sns.heatmap(new_df.select_dtypes(np.number).corr(), 
            annot=True,
            cbar=False,
            cmap="YlGnBu",
            xticklabels=new_df.select_dtypes(np.number).columns,
            yticklabels=new_df.select_dtypes(np.number).columns)
plt.title('Correlation of Numeric Features')
plt.show()

"""Jika diamati, fitur repeat_retailer memiliki korelasi sangat rendah dengan fitur target yaitu fraud. Sehingga, fitur tersebut dapat di-drop."""

new_df.drop(['repeat_retailer'], inplace=True, axis=1)
new_df.head()

"""# Data Preparation

## Train Test Split
"""

new_X = new_df.drop('fraud', axis=1)
new_y = new_df['fraud']

new_x_train, new_x_test, new_y_train, new_y_test = train_test_split(new_X, new_y, test_size=0.2, random_state=42)

x_test.drop(['repeat_retailer'], inplace=True, axis=1)

print(f'Total sample in train dataset: {len(new_x_train)}')
print(f'Total sample in test dataset: {len(new_x_test)}')

"""## Stardardization"""

scaler = MinMaxScaler()
scaler.fit(new_x_train)
new_x_train = scaler.transform(new_x_train)
x_test = scaler.transform(x_test)

"""# Modeling

Disini saya akan melakukan perbandingan antara dua algoritma, yaitu:
- Naive Bayes
- Logistic Regression

## Naive Bayes
"""

from sklearn.naive_bayes import GaussianNB

bayes = GaussianNB()
bayes.fit(new_x_train, new_y_train)
model_bayes = bayes.predict(x_test)

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
 
lr = LogisticRegression()
lr.fit(new_x_train, new_y_train)
model_lr = lr.predict(x_test)

"""# Evaluation"""

def evaluate_classifier_performance(classifier, prediction, y_test = y_test, x_test = x_test):
    print("Hasil Evaluasi:\n\n%s\n" % (classification_report(y_test, classifier.predict(x_test))))

    print('Accuracy:', accuracy_score(y_test, prediction))
    print('Precision Macro Average:', precision_score(y_test, prediction, average='macro'))
    print('Precision Micro Average:', precision_score(y_test, prediction, average='micro'))
    print('Recall Macro Average:', recall_score(y_test, prediction, average='macro'))
    print('Recall Micro Average:', recall_score(y_test, prediction, average='micro'))
    print('F1 Macro Average:', f1_score(y_test, prediction, average='macro'))
    print('F1 Micro Average:', f1_score(y_test, prediction, average='micro'))

    print("\n\nConfusion Matrix\n")
    ConfusionMatrixDisplay.from_estimator(classifier, x_test, y_test)

evaluate_classifier_performance(bayes, model_bayes)

evaluate_classifier_performance(lr, model_lr)